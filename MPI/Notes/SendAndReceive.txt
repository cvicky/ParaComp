
http://mpitutorial.com/tutorials/mpi-send-and-receive/

///////////////////////////////////////
Blocking Point-to-Point Communication
///////////////////////////////////////

===========================================
MPI Send and Receive

-almost every single function in MPI can be implemented with basic send and receive calls

Overview:
-blocking and receiving functions
-other basic concepts associated with transmitting data using MPI

Sending and Receiving
-first, process A decides a message needs to be sent to process B
-process A then packs all of its necessary data into a buffer for process B
	-buffers are often referred to as envelopes (messages are packed into them)
-after data is packed into buffer, the communication device (network) is responsible for routing the message to proper location
	-location of message is defined by process’s rank

-even though message is routed to B, process B still needs to acknowledge that it wants to receive A’s data in the first place (YEA, CONSENT)
-once B acknowledges that it wants to receive A’s data, the data has been transmitted 
	-data may even go back and forth

-sometimes A might have to send different types of messages to B
-instead of B having to differentiate all these messages, MPI allows sender and receivers to specify message IDs (aka tags)
-when process B requests a message with a certain tag number, messages with different tags will be buffered by the network until B is ready for them

Functions:

MPI_Send(
    void* data,
    int count,
    MPI_Datatype datatype,
    int destination,
    int tag,
    MPI_Comm communicator)

MPI_Recv(
    void* data,
    int count,
    MPI_Datatype datatype,
    int source,
    int tag,
    MPI_Comm communicator,
    MPI_Status* status)

-first arg is the data buffer
-2nd and 3rd are the count and type of elements that reside in the butter
-MPI_Send will send the exact count of elements
-MPI_Recv will receive AT MOST the count of elements
-4th and 5th args specify the rank of the sending/receiving process and the tag of the message
-6th arg specifies the communicator
-last arg (for MPI_Recv ONLY) provides info about the received message

Elementary MPI datatypes

MPI datatype		C equivalent
MPI_SHORT		short int
MPI_INT			int
MPI_LONG		long int
MPI_LONG_LONG		long long int
MPI_UNSIGNED_CHAR	unsigned char
MPI_UNSIGNED_SHORT	unsigned short int
MPI_UNSIGNED		unsigned int
MPI_UNSIGNED_LONG	unsigned long int
MPI_UNSIGNED_LONG_LONG	unsigned long long int
MPI_FLOAT		float
MPI_DOUBLE		double
MPI_LONG_DOUBLE		long double
MPI_BYTE		char

MPI send/recv program
————————————————————————————————
// Find out rank, size
int world_rank;
MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
int world_size;
MPI_Comm_size(MPI_COMM_WORLD, &world_size);

int number;
if (world_rank == 0) {
    number = -1;
    MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
} else if (world_rank == 1) {
    MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD,
             MPI_STATUS_IGNORE);
    printf("Process 1 received number %d from process 0\n",
           number);
}
//output: Process 1 received number -1 from process 0
—————————————————————————————

-MPI_Comm_rank is used to determine the rank of the process
-MPI_Comm_size is used to determine the world size 
-then the process zero initializes a number to the value of -1 and send this value to process one 

MPI Ping Pong Program
-processes use MPI_Send and MPI_Recv to continually bounce messages off of each other 

Program portion:
————————————
int ping_pong_count = 0;
int partner_rank = (world_rank + 1) % 2;
while (ping_pong_count < PING_PONG_LIMIT) {
    if (world_rank == ping_pong_count % 2) {
        // Increment the ping pong count before you send it
        ping_pong_count++;
        MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0,
                 MPI_COMM_WORLD);
        printf("%d sent and incremented ping_pong_count "
               "%d to %d\n", world_rank, ping_pong_count,
               partner_rank);
    } else {
        MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0,
                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("%d received ping_pong_count %d from %d\n",
               world_rank, ping_pong_count, partner_rank);
    }
}
—————————————
//output:
0 sent and incremented ping_pong_count 1 to 1
0 received ping_pong_count 2 from 1
0 sent and incremented ping_pong_count 3 to 1
0 received ping_pong_count 4 from 1
0 sent and incremented ping_pong_count 5 to 1
0 received ping_pong_count 6 from 1
0 sent and incremented ping_pong_count 7 to 1
0 received ping_pong_count 8 from 1
0 sent and incremented ping_pong_count 9 to 1
0 received ping_pong_count 10 from 1
1 sent and incremented ping_pong_count 1 to 0
1 received ping_pong_count 2 from 0
1 sent and incremented ping_pong_count 3 to 0
1 received ping_pong_count 4 from 0
1 sent and incremented ping_pong_count 5 to 0
1 received ping_pong_count 6 from 0
1 sent and incremented ping_pong_count 7 to 0
1 received ping_pong_count 8 from 0
1 sent and incremented ping_pong_count 9 to 0
1 received ping_pong_count 10 from 0

——————————————

-ping_pong_count is initialized to 0, incremented at each ping pong step by the sending process
-as ping_pong_count is incremented, the processes take turns being sender and receiver
-after ping_pong_limit is reached, the processed will stop sending and receiving
-output may look different on other machines b/c of process scheduling

MPI Ring Problem
-this uses more than 2 processes
-value is passes around by all processes in a ring-like fashion

——————————
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv) {
  // Initialize the MPI environment
  MPI_Init(NULL, NULL);
  // Find out rank, size
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  int token;
  // Receive from the lower process and send to the higher process. Take care
  // of the special case when you are the first process to prevent deadlock.
  if (world_rank != 0) {
    MPI_Recv(&token, 1, MPI_INT, world_rank - 1, 0, MPI_COMM_WORLD,
             MPI_STATUS_IGNORE);
    printf("Process %d received token %d from process %d\n", world_rank, token,
           world_rank - 1);
  } else {
    // Set the token's value if you are process 0
    token = -1;
  }
  MPI_Send(&token, 1, MPI_INT, (world_rank + 1) % world_size, 0,
           MPI_COMM_WORLD);
  // Now process 0 can receive from the last process. This makes sure that at
  // least one MPI_Send is initialized before all MPI_Recvs (again, to prevent
  // deadlock)
  if (world_rank == 0) {
    MPI_Recv(&token, 1, MPI_INT, world_size - 1, 0, MPI_COMM_WORLD,
             MPI_STATUS_IGNORE);
    printf("Process %d received token %d from process %d\n", world_rank, token,
           world_size - 1);
  }
  MPI_Finalize();
}
——————————
//output: with 5 processes
Process 1 received token -1 from process 0
Process 2 received token -1 from process 1
Process 3 received token -1 from process 2
Process 4 received token -1 from process 3
Process 0 received token -1 from process 4
—————————
-process 0 first sends -1 to process 1
-then -1 gets passed around the ring until it gets back to process 0 

-ring program initializes a value from process 0, and the value of passed around every single process
-program terminates when process 0 receives the value from the last process
-extra care is taken so that there is no deadlock
	-process 0 makes sure that it has completed its first send before it tries to receive the value from the last process
-all other processes call MPI_Recv (receiving from their neighboring lower processes) and then MPI_Send (sending the value to their neighboring higher processes) to pass the value along the ring

============================================

http://mpitutorial.com/tutorials/dynamic-receiving-with-mpi-probe-and-mpi-status/

Dynamic Receiving with MPI_probe and MPI_Status

-earlier programs were sending messages in which the length of the message was known beforehand

****-MPI natively supports dynamic messages with just a few addition function calls

MPI Status Structure
-the MPI_Recv operation takes the address of an MPI_Status

